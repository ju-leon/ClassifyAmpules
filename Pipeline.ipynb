{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Ampules\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Erstmal müssen wir unser Repository in unsere Colab Instanz laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1EhG5b5fa0ti",
    "outputId": "6c485a4a-3cf0-458d-9ab2-6fdc1b41e549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ClassifyAmpules' already exists and is not an empty directory.\n",
      "/content/ClassifyAmpules\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ju-leon/ClassifyAmpules.git\n",
    "%cd ClassifyAmpules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive\n",
    "\n",
    "Wir verwenden **Google Drive** um Daten zu laden und zu speichern.\n",
    "Zum Einbinden von Google Drive dem Link unten folgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "CGNM2C0oc5Kr",
    "outputId": "f766194f-b762-41b9-fbd9-097ab2510337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSbYSqemEBtM"
   },
   "source": [
    "### Updating\n",
    "\n",
    "Um Änderungen in unserem Repositiory sichbar zu machen muss es neu geladen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qZnM1PvD-gI"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GxMmEPXDv4f"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Hier setzen wir unser Dateisystem auf.\n",
    "\n",
    "**Source Path:** Pfad zu den Rohdaten\n",
    "\n",
    "**Cache Path:** Pfad in ein leeren Ordner. Hier werden Bilder nach der ersten Preprocessing Schritt gespeichert.\n",
    "\n",
    "**Out Path:** Pfad in ein leeren Ordner. Hier werden die fertig verarbeiteten Bilder gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCa2J4tTbggc"
   },
   "outputs": [],
   "source": [
    "source_path = \"/content/drive/My Drive/Hackaton/PIM_Dataset\"\n",
    "cache_path = \"/content/drive/My Drive/Hackaton/PIM_combi\"\n",
    "out_path =  \"/content/drive/My Drive/Hackaton/combined\"\n",
    "\n",
    "source_path_good = \"'\" + source_path + \"/good_piece/\" + \"'\"\n",
    "source_path_bad = \"'\" + source_path + \"/bad_piece\" + \"'\"\n",
    "\n",
    "cache_path_good = \"'\" + cache_path + \"/good_piece/\" + \"'\"\n",
    "cache_path_bad = \"'\" + cache_path + \"/bad_piece/\" + \"'\"\n",
    "\n",
    "out_path_good = \"'\" + out_path + \"/good_piece/\" + \"'\"\n",
    "out_path_bad = \"'\" + out_path + \"/bad_piece/\" + \"'\"\n",
    "out_path = \"'\" + out_path + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls die (leeren) Ordner noch nicht existieren erstellen wir sie..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p $cache_path_good $cache_path_bad $out_path_good $out_path_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pd3kU_Ola0tn"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "![Raw Data](docs/metal226_3.bmp)\n",
    "\n",
    "Als erstes müssen wir die Datenmenge reduzieren.\n",
    "\n",
    "Wir verwenden einen **min Filter** der Größe 9 bevor wir runter skalieren. \n",
    "Dadurch werden Abweichungen von dem Hintergrund wie Metallteile hervorgehoben und vergrößert. Das verhindert, das diese Merkmale beim runterskalieren verloren gehen.\n",
    "Hier sieht man, dass das Metallstück rechts unten deutlich größer erscheint und somit beim runterskalieren noch deutlich besser sichtbar ist.\n",
    "\n",
    "\n",
    "![Raw Data](docs/after_preprocess.png)\n",
    "\n",
    "Danach scheiden wir den wichtigen Bildbereich aus.\n",
    "Mit mehr Zeit könnte man dieses Ausschneiden intelligent machen, das also immer genau die Ampullen ausgeschnitten werden.\n",
    "Das würde die Datenmenge deutlich reduzieren, die Variation zwischen den Bildern minimieren, und damit höchstwahrscheinlich das Klassifikationsergebniss um ein Vielfaches verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YU5UtGG2a0to"
   },
   "outputs": [],
   "source": [
    "!python3 preprocess.py $source_path_good $cache_path_good\n",
    "!python3 preprocess.py $source_path_bad $cache_path_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3_VjgOTa0ts"
   },
   "source": [
    "# Combining\n",
    "\n",
    "Für jedes Sample werden 3 Bilder angegeben. \n",
    "Um ein Overfitting zu vermeiden, müssen diese Bilder kombiniert werden, sonst könnte das Netz allein anhand der Ausrichtung der Ampullen erkennen ob sie Verunreinigt sind. Das würde die Metriken bei Training nutzlos machen.\n",
    "\n",
    "Deshab legen wir alle 3 Bilder übereinander. Wir nehmen für jedes Pxxel das mit dem dunkelsten Wert.\n",
    "Dadurch bleiben wichtige Merkmale wie Metallstücke erhalten, doppelte Info wie Ränder verschwinden.\n",
    "\n",
    "![Raw Data](docs/metal226.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hb-94e0na0tt",
    "outputId": "56df6e59-b75c-4c90-d354-f82a2de314f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 1680/1680 [00:00<00:00, 990501.93it/s]\n",
      "100% 560/560 [01:50<00:00,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 combine.py $cache_path_good $out_path_good\n",
    "!python3 combine.py $cache_path_bad $out_path_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwsoNCHrOsCv"
   },
   "source": [
    "# Filter by bad piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEPnV1ZBLVGJ"
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os\n",
    "path = \"/content/drive/My Drive/Hackaton/combined/good_piece/\"\n",
    "\n",
    "valid_images = \".png\"\n",
    "for f in os.listdir(path):\n",
    "    if f.endswith(valid_images):\n",
    "        copyfile(path + f, \"/content/drive/My Drive/Hackaton/all_classes/good/\" + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47W-zxewa0tw"
   },
   "source": [
    "# Training\n",
    "\n",
    "Jetzt können wir das Netz trainieren. \n",
    "\n",
    "\n",
    "Wir verwenden Tensorflow und die high-level API Keras zum Trainieren eines neuronalen Netztes.\n",
    "Bei unserer Architektur, setzen wir angelehnt an Imagenet (vgl. https://arxiv.org/abs/1409.0575)\n",
    "auf ein CNN (Convolutional Neural Network) das mehrere Convolutional Blocks (Convolutional Layer + Max Pooling Layer) beinhaltet, die mit Relu aktiviert werden. Darauf folgt ein Dropout und darauf 3 Fully Connected Layer um zu klassifizieren. Der finale Output wird durch einen Softmax-Layer erzeugt. \n",
    "\n",
    "Dieser erzeugt Pseudowahrscheinlichkeiten für die Zugehörigkeit eines gebenen Sample zur jeweiligen Klasse (Good Piece, Bad Piece).\n",
    "Diese Architektur hat sich zum Klassifizieren von Bildern bewährt, da Features durch Convolutional Layer gelernt werden und dann durch die Fully Connected Layer klassifiziert werden können.\n",
    "\n",
    "Wir haben auch Tests mit einem Netzwerk das auf metal, gasket, charred, good gemacht. Dabei haben wir ein accurcy von 75% erreicht, webei die meisten Fehler innerhalb der \"bad-Klassen\" erfolgt sind. die Fehler wären also nicht besonders schlimm, da nur der grund für die Ablehnung falsch klassifiziert wurde, aber nur selten fälschlicherweise eine defekte Charge als gut klassifiziert wurde. \n",
    "Dennoch ist insgesamt die Fehlerhäufigkeit höher, was auch an der sehr geringen Datenmenge pro Klasse liegen könnte wenn man die \"bad_piece\" Klasse noch weit\n",
    "\n",
    "\n",
    "Unsere Daten werden geshuffelt und in Test und Validierungsdaten aufgeteilt (Verhältnis 70%:30%).\n",
    "Wir verwenden ADAM (vgl. https://arxiv.org/abs/1412.6980) als Optimierungsalgorithmus und validieren die Optimierung nach jeder Trainingsepoche mit unserem Validierungsdatensatz. Verbessert sich die Genauigkeit des Netzes wird dieses gespeichert. Am Ende des Training werden mit dem besten Netz auf dem Validierungsdatensatz die Confusion Matrix und Metriken wie f1-score, precision, ... erzeugt.\n",
    "\n",
    "## Anmerkung zu Hyperparameteroptimierung\n",
    "Parameter wie Lernrate, Anzahl der Schichten, Größe der Schichten, Größe der Convultional Filter usw. können durch Systematische Ansätze der Hyperparameteroptimierung (wie z.B Neural Architecture Search) immer weiter Optimiert werden. Solche Ansätze sprengen leider den Zeitlichen Rahmen eines Hackathons, führen aber, auf Grundlage der von uns geschaffenen Architektur, zu exzellenten Ergebnissen. Die weiter Optimierung wäre für den Produktiveinsatz wünschenswert.\n",
    "\n",
    "\n",
    "TODO: Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ijLNnPtha0tw",
    "outputId": "4a84496f-7896-49d4-8ad6-e6206ea321d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2020-07-02 21:36:30.177344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "Found 2677 images belonging to 2 classes.\n",
      "Found 1147 images belonging to 2 classes.\n",
      "(100, 100, 3)\n",
      "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n",
      "2020-07-02 21:36:32.532718: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-02 21:36:32.572278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:32.572942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2020-07-02 21:36:32.572983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-02 21:36:32.818590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-02 21:36:32.950707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-02 21:36:32.975811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-02 21:36:33.263983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-02 21:36:33.309049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-02 21:36:33.822471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-02 21:36:33.822699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:33.823458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:33.823993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-07-02 21:36:33.846917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n",
      "2020-07-02 21:36:33.847126: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ac5480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-02 21:36:33.847156: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-02 21:36:33.996097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:33.997050: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ac5d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-02 21:36:33.997093: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-07-02 21:36:33.998658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:33.999430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2020-07-02 21:36:33.999493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-02 21:36:33.999539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-02 21:36:33.999567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-02 21:36:33.999586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-02 21:36:33.999604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-02 21:36:33.999622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-02 21:36:33.999643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-02 21:36:33.999762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:34.000604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:34.001345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-07-02 21:36:34.005908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-02 21:36:40.422798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-02 21:36:40.422862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2020-07-02 21:36:40.422879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2020-07-02 21:36:40.427566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:40.428263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-07-02 21:36:40.428825: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-07-02 21:36:40.428877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n",
      "2020-07-02 21:36:43.628662: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2020-07-02 21:36:43.630128: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs\n",
      "2020-07-02 21:36:43.671982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\n",
      "2020-07-02 21:36:43.855514: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed\n",
      "Epoch 1/100\n",
      "2020-07-02 21:37:05.552897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-02 21:37:07.038608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      " 1/27 [>.............................] - ETA: 8:28 - loss: 2.0851 - accuracy: 0.50002020-07-02 21:37:11.859516: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      " 2/27 [=>............................] - ETA: 4:05 - loss: 1.8431 - accuracy: 0.51562020-07-02 21:37:11.961376: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed\n",
      "2020-07-02 21:37:11.962135: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 3468 callback api events and 3468 activity events.\n",
      "2020-07-02 21:37:12.120235: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12\n",
      "2020-07-02 21:37:12.193118: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12/7ee58e74782b.trace.json.gz\n",
      "2020-07-02 21:37:12.236010: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 1.192 ms\n",
      "\n",
      "2020-07-02 21:37:12.257290: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12Dumped tool data for overview_page.pb to logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12/7ee58e74782b.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12/7ee58e74782b.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12/7ee58e74782b.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/1593725803.6285982/train/plugins/profile/2020_07_02_21_37_12/7ee58e74782b.kernel_stats.pb\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.162361). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "27/27 [==============================] - 242s 9s/step - loss: 0.9610 - accuracy: 0.6701 - val_loss: 1.7470 - val_accuracy: 0.5398\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53977, saving model to /content/drive/My Drive/Hackaton/nets/final.h5\n",
      "Epoch 2/100\n",
      " 4/27 [===>..........................] - ETA: 3:25 - loss: 0.4111 - accuracy: 0.8672"
     ]
    }
   ],
   "source": [
    "!python3 train.py $out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVfF4Khga0tz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Kopie von Pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
